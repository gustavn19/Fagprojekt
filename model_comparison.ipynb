{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess, tokenize\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "import gensim.corpora as corpora\n",
    "from nltk.stem.porter import *\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string \n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "import time\n",
    "np.random.seed(69)\n",
    "random.seed(69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jacob pc\\AppData\\Local\\Temp\\ipykernel_6732\\1787380598.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_kw['index'] = data_kw.index\n",
      "100%|██████████| 172392/172392 [00:51<00:00, 3327.81it/s]\n"
     ]
    }
   ],
   "source": [
    "data=pd.read_csv(\"FAOLEX_All.csv\")\n",
    "# print(data.keys())\n",
    "# print(df.info())\n",
    "# print(keywords[:5])\n",
    "\n",
    "# Create simplified version of the dataframe by discarding irrelevant columns \n",
    "to_drop = ['Record Id', \n",
    "           'Record URL', \n",
    "           'Document URL', \n",
    "           'Title',\n",
    "           'Original title',\n",
    "           'Available website',\n",
    "           'Abstract']\n",
    "\n",
    "data.drop(to_drop, inplace=True, axis=1)\n",
    "# Remove NaN values in the Countries/Territory and Keywords attributes \n",
    "data_no_nan = data.dropna(subset=[\"Country/Territory\", \"Keywords\"])\n",
    "# Remove all documents that have been repealed (Repealed == Y)\n",
    "data_no_repealed = data_no_nan[data_no_nan.Repealed != 'Y']\n",
    "\n",
    "data_kw = data_no_repealed[[\"Country/Territory\",\"Keywords\"]]\n",
    "data_kw['index'] = data_kw.index\n",
    "keywords = data_kw\n",
    "\n",
    "#preproccessing\n",
    "tokenizer = RegexpTokenizer(r'\\w+') # create tokenizer to tokenize docs \n",
    "stop_words = stopwords.words('english') # create list of stopwords\n",
    "p_stemmer = PorterStemmer() # create stemmer instance \n",
    "\n",
    "doc_set = keywords[\"Keywords\"].tolist()\n",
    "processed_kw_list = []\n",
    "\n",
    "# loop through document list\n",
    "for i in tqdm(doc_set):\n",
    "    \n",
    "    # clean and tokenize document string\n",
    "    raw = i.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens = [i for i in tokens if not i in stop_words]\n",
    "    \n",
    "    # stem tokens\n",
    "    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    \n",
    "    # add tokens to list\n",
    "    processed_kw_list.append(stemmed_tokens)\n",
    "    #processed_kw_list.append(stopped_tokens)\n",
    "    \n",
    "country = keywords[\"Country/Territory\"].tolist()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping from word IDs to words\n",
    "# https://medium.com/analytics-vidhya/topic-modeling-using-gensim-lda-in-python-48eaa2344920\n",
    "id2word = corpora.Dictionary(processed_kw_list)\n",
    "\n",
    "# Prepare Document-Term matrix\n",
    "corpus = []\n",
    "for doc in processed_kw_list:\n",
    "    corpus.append(id2word.doc2bow(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jacob pc\\anaconda3\\lib\\site-packages\\umap\\distances.py:1063: NumbaDeprecationWarning: \u001B[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001B[0m\n",
      "  @numba.jit()\n",
      "C:\\Users\\Jacob pc\\anaconda3\\lib\\site-packages\\umap\\distances.py:1071: NumbaDeprecationWarning: \u001B[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001B[0m\n",
      "  @numba.jit()\n",
      "C:\\Users\\Jacob pc\\anaconda3\\lib\\site-packages\\umap\\distances.py:1086: NumbaDeprecationWarning: \u001B[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001B[0m\n",
      "  @numba.jit()\n",
      "C:\\Users\\Jacob pc\\anaconda3\\lib\\site-packages\\umap\\umap_.py:660: NumbaDeprecationWarning: \u001B[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001B[0m\n",
      "  @numba.jit()\n"
     ]
    }
   ],
   "source": [
    "# Load models\n",
    "from top2vec import Top2Vec\n",
    "from gensim import  models\n",
    "top2vec_model = Top2Vec.load(\"top2vec_model\")\n",
    "lda_model = models.ldamodel.LdaModel.load(\"lda_model\")\n",
    "n_topics=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics \n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vectors_top2vec = []\n",
    "feature_vectors_lda = []\n",
    "country_all = []\n",
    "for i in (range(len(country))):\n",
    "    if len(country[i].split(\"; \"))>=2:\n",
    "        for j in range(len(country[i].split(\"; \"))):\n",
    "            #top2vec\n",
    "            topic_nums,topic_score,_,_ = top2vec_model.get_documents_topics([i], reduced=False, num_topics=top2vec_model.get_num_topics())\n",
    "            feature_vectors_top2vec.append(topic_score[0][topic_nums[0].argsort()])\n",
    "            #LDA\n",
    "            topic_probs = lda_model.get_document_topics(corpus[i], minimum_probability=0)\n",
    "            probs = [t[-1] for t in topic_probs]\n",
    "            feature_vectors_lda.append(probs)\n",
    "            #\n",
    "            country_all.append(country[i].split(\"; \")[j])\n",
    "    else:\n",
    "        #top2ved\n",
    "        topic_nums,topic_score,_,_ = top2vec_model.get_documents_topics([i], reduced=False, num_topics=top2vec_model.get_num_topics())\n",
    "        feature_vectors_top2vec.append(topic_score[0][topic_nums[0].argsort()])\n",
    "        #LDA\n",
    "        topic_probs = lda_model.get_document_topics(corpus[i], minimum_probability=0)\n",
    "        probs = [t[-1] for t in topic_probs]\n",
    "        feature_vectors_lda.append(probs)\n",
    "        #\n",
    "        country_all.append(country[i])\n",
    "labels=country_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jacob pc\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=4.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Stratified CV: \n",
    "X_top2vec = np.array(feature_vectors_top2vec)\n",
    "X_lda = np.array(feature_vectors_lda)\n",
    "y = np.array(labels)\n",
    "neigh_top2vec = KNeighborsClassifier(n_neighbors=5)\n",
    "neigh_lda = KNeighborsClassifier(n_neighbors=5)\n",
    "kfold = StratifiedKFold(n_splits=4, shuffle=True, random_state=11)\n",
    "accu_stratified_lda = []\n",
    "accu_stratified_top2vec = []\n",
    "accu_stratified_baseline = []\n",
    "for train_index, test_index in kfold.split(X_top2vec, y):\n",
    "    x_train_top2vec, x_test_top2vec = X_top2vec[train_index], X_top2vec[test_index]\n",
    "    x_train_lda, x_test_lda = X_lda[train_index], X_lda[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    neigh_top2vec.fit(x_train_top2vec, y_train)\n",
    "    neigh_lda.fit(x_train_lda, y_train)\n",
    "    accu_stratified_top2vec.append(metrics.accuracy_score(y_test, neigh_top2vec.predict(x_test_top2vec)))\n",
    "    accu_stratified_lda.append(metrics.accuracy_score(y_test, neigh_lda.predict(x_test_lda)))\n",
    "    # Baseline\n",
    "    counter = Counter(y_train)\n",
    "    most_common_element, occurrence_count = counter.most_common(1)[0]\n",
    "    accu_stratified_baseline.append(occurrence_count/len(feature_vectors_lda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean accuracy lda 0.21993195119154513\n",
      "mean accuracy top2vec 0.18370676267392294\n",
      "Baseline accuracy 0.03838000435707942\n",
      "Paired t-test:\n",
      "Test statistic: 27.974258286262817\n",
      "p-value: 0.00010027685978436129\n"
     ]
    }
   ],
   "source": [
    "print(\"mean accuracy lda\", np.mean(accu_stratified_lda))\n",
    "print(\"mean accuracy top2vec\", np.mean(accu_stratified_top2vec))\n",
    "print(\"Baseline accuracy\", np.mean(accu_stratified_baseline))\n",
    "\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "t_statistic, p_value = stats.ttest_rel(accu_stratified_lda, accu_stratified_top2vec)\n",
    "\n",
    "# Print the test statistic and p-value\n",
    "print(\"Paired t-test:\")\n",
    "print(\"Test statistic:\", t_statistic)\n",
    "print(\"p-value:\", p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(feature_vectors_top2vec, labels, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh1 = KNeighborsClassifier(n_neighbors=5)\n",
    "neigh1.fit(X_train, y_train)\n",
    "y_pred_knn = neigh1.predict(X_test)\n",
    "print(\"ACCURACY OF THE MODEL: \", metrics.accuracy_score(y_test, y_pred_knn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bedste n_neighbors - top2vec: 1(bedst 1.), 5(3.), 8(2.), 15(lidt dårligere) 25 (meget dårligere)\n",
    "Bedste n_neighbors - lda: 1 (19.6), 3 ( 17.38) 5(18), 25 (17.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mcnemars test\n",
    "from scipy.stats import beta\n",
    "from scipy.stats import binom\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "split_shuffle = ShuffleSplit(n_splits=1, random_state=11, test_size=0.3)\n",
    "X_top2vec = np.array(feature_vectors_top2vec)\n",
    "X_lda = np.array(feature_vectors_lda)\n",
    "y = np.array(labels)\n",
    "for train_index, test_index in split_shuffle.split(X_top2vec, y):\n",
    "    x_train_top2vec, x_test_top2vec = X_top2vec[train_index], X_top2vec[test_index]\n",
    "    x_train_lda, x_test_lda = X_lda[train_index], X_lda[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    d1_n_11=[]\n",
    "    d1_n_10=[]\n",
    "    d1_n_01=[]\n",
    "    d1_n_00=[]\n",
    "\n",
    "    d2_n_11=[]\n",
    "    d2_n_10=[]\n",
    "    d2_n_01=[]\n",
    "    d2_n_00=[]\n",
    "\n",
    "    d3_n_11=[]\n",
    "    d3_n_10=[]\n",
    "    d3_n_01=[]\n",
    "    d3_n_00=[]\n",
    "\n",
    "    lda_score=0\n",
    "    top2vec_score=0\n",
    "    base_score=0\n",
    "    i=0\n",
    "    # top2vec\n",
    "    neigh_top2vec = KNeighborsClassifier(n_neighbors=1)\n",
    "    neigh_top2vec.fit(x_train_top2vec, y_train)\n",
    "    top2vec_score=neigh_top2vec.predict(x_test_top2vec) == y_test\n",
    "    print(\"ACCURACY OF THE top2vec MODEL: \", metrics.accuracy_score(y_test, neigh_top2vec.predict(x_test_top2vec)))\n",
    "    #LDA\n",
    "    neigh_lda = KNeighborsClassifier(n_neighbors=1)\n",
    "    neigh_lda.fit(x_train_lda, y_train)\n",
    "    lda_score=neigh_lda.predict(x_test_lda) == y_test\n",
    "    print(\"ACCURACY OF THE LDA MODEL: \", metrics.accuracy_score(y_test, neigh_lda.predict(x_test_lda)))\n",
    "    # Baseline\n",
    "    counter = Counter(y_train)\n",
    "    most_common_element, occurrence_count = counter.most_common(1)[0]\n",
    "    base_score = most_common_element == y_test\n",
    "\n",
    "    # LDA vs Top2vec\n",
    "    #Both are corect\n",
    "    d1_n_11.append(sum(lda_score*top2vec_score))\n",
    "    # LDA is correct Top is wrong\n",
    "    d1_n_10.append(sum(lda_score*(1-top2vec_score)))\n",
    "    # LDA is wrong Top is correct\n",
    "    d1_n_01.append(sum((1-lda_score)*top2vec_score))\n",
    "    #Both wrong\n",
    "    d1_n_00.append(sum((1-lda_score)*(1-top2vec_score)))\n",
    "    # difference in accuracy\n",
    "    n=len(y_test)\n",
    "    Etheta=(d1_n_10[i]-d1_n_01[i])/n\n",
    "    if (n*(d1_n_10[i]+d1_n_01[i])-(d1_n_10[i]-d1_n_01[i])**2)!=0:\n",
    "        Q=n**2*(n+1)*(Etheta+1)*(1-Etheta)/(n*(d1_n_10[i]+d1_n_01[i])-(d1_n_10[i]-d1_n_01[i])**2)\n",
    "        f=(Etheta+1)/2*(Q-1)\n",
    "        g=(1-Etheta)/2*(Q-1)\n",
    "        # 95 % confidence interval\n",
    "        p_u=2*beta.ppf(0.975,f,g)-1\n",
    "        p_l=2*beta.ppf(0.025,f,g)-1\n",
    "        print(\"LDA vs Top2Vec \")\n",
    "        print(\"CI: \",p_l,\":\",p_u) \n",
    "    else:\n",
    "        print(\"ZeroDivisionError\")\n",
    "\n",
    "    # P- value\n",
    "    pval=2*binom.cdf(min(d1_n_10[i],d1_n_01[i]),(d1_n_10[i]+d1_n_01[i]),0.5)\n",
    "    print(\"p-value: \", pval)\n",
    "\n",
    "    #Sammenligniner med baseline\n",
    "\n",
    "    #LDA vs Baseline\n",
    "    #Both are corect\n",
    "    d2_n_11.append(sum(lda_score*base_score))\n",
    "    # LDA is correct Top is wrong\n",
    "    d2_n_10.append(sum(lda_score*(1-base_score)))\n",
    "    # LDA is wrong Top is correct\n",
    "    d2_n_01.append(sum((1-lda_score)*base_score))\n",
    "    #Both wrong\n",
    "    d2_n_00.append(sum((1-lda_score)*(1-base_score)))\n",
    "    # difference in accuracy\n",
    "    n=len(y_test)\n",
    "    Etheta=(d2_n_10[i]-d2_n_01[i])/n\n",
    "    if (n*(d2_n_10[i]+d2_n_01[i])-(d2_n_10[i]-d2_n_01[i])**2)!=0:\n",
    "        Q=n**2*(n+1)*(Etheta+1)*(1-Etheta)/(n*(d2_n_10[i]+d2_n_01[i])-(d2_n_10[i]-d2_n_01[i])**2)\n",
    "        f=(Etheta+1)/2*(Q-1)\n",
    "        g=(1-Etheta)/2*(Q-1)\n",
    "        # 95 % confidence interval\n",
    "        p_u=2*beta.ppf(0.975,f,g)-1\n",
    "        p_l=2*beta.ppf(0.025,f,g)-1\n",
    "        print(\"LDA vs baseline \")\n",
    "        print(\"CI: \",p_l,\":\",p_u) \n",
    "    else:\n",
    "        print(\"ZeroDivisionError\")\n",
    "\n",
    "    # P- value\n",
    "    pval=2*binom.cdf(min(d2_n_10[i],d2_n_01[i]),(d2_n_10[i]+d2_n_01[i]),0.5)\n",
    "    print(\"p-value: \", pval)\n",
    "\n",
    "     #Top2vec vs Baseline\n",
    "    #Both are corect\n",
    "    d3_n_11.append(sum(top2vec_score*base_score))\n",
    "    # LDA is correct Top is wrong\n",
    "    d3_n_10.append(sum(top2vec_score*(1-base_score)))\n",
    "    # LDA is wrong Top is correct\n",
    "    d3_n_01.append(sum((1-top2vec_score)*base_score))\n",
    "    #Both wrong\n",
    "    d3_n_00.append(sum((1-top2vec_score)*(1-base_score)))\n",
    "    # difference in accuracy\n",
    "    n=len(y_test)\n",
    "    Etheta=(d3_n_10[i]-d3_n_01[i])/n\n",
    "    if (n*(d3_n_10[i]+d3_n_01[i])-(d3_n_10[i]-d3_n_01[i])**2)!=0:\n",
    "        Q=n**2*(n+1)*(Etheta+1)*(1-Etheta)/(n*(d3_n_10[i]+d3_n_01[i])-(d3_n_10[i]-d3_n_01[i])**2)\n",
    "        f=(Etheta+1)/2*(Q-1)\n",
    "        g=(1-Etheta)/2*(Q-1)\n",
    "        # 95 % confidence interval\n",
    "        p_u=2*beta.ppf(0.975,f,g)-1\n",
    "        p_l=2*beta.ppf(0.025,f,g)-1\n",
    "        print(\"Top2vec vs baseline \")\n",
    "        print(\"CI: \",p_l,\":\",p_u) \n",
    "    else:\n",
    "        print(\"ZeroDivisionError\")\n",
    "\n",
    "    # P- value\n",
    "    pval=2*binom.cdf(min(d3_n_10[i],d3_n_01[i]),(d3_n_10[i]+d3_n_01[i]),0.5)\n",
    "    print(\"p-value: \", pval)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1_n_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1_n_01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1_n_11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1_n_00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_neigh =[3,5,7,9,11,13]\n",
    "x_train_top2vec, x_test_top2vec, y_train_top2vec, y_test_top2vec = train_test_split(feature_vectors_top2vec, labels, test_size = 0.3)\n",
    "x_train_lda, x_test_lda, y_train_lda, y_test_lda = train_test_split(feature_vectors_lda, labels, test_size = 0.3)\n",
    "\n",
    "\n",
    "for n in n_neigh:\n",
    "    neigh1 = KNeighborsClassifier(n_neighbors=n)\n",
    "    neigh1.fit(x_train_top2vec, y_train_top2vec)\n",
    "    y_pred_knn = neigh1.predict(x_test_top2vec)\n",
    "    print(\"ACCURACY OF top2vec MODEL with\",n , \" neighbors: \", metrics.accuracy_score(y_test_top2vec, y_pred_knn))\n",
    "\n",
    "    neigh2 = KNeighborsClassifier(n_neighbors=n)\n",
    "    neigh2.fit(x_train_lda, y_train_lda)\n",
    "    y_pred_knn = neigh2.predict(x_test_lda)\n",
    "    print(\"ACCURACY OF LDA MODEL with\",n , \" neighbors: \", metrics.accuracy_score(y_test_lda, y_pred_knn))"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16 ('top2vec': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5f35e34d110ddf77291a333d8873e10bca0fc5162ed588aa5dded51d6b040b88"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
