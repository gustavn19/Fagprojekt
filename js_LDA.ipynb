{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important standard packages \n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing packages used in data pre-processing\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess, tokenize\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "import gensim.corpora as corpora\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing data\n",
    "1. Loading data and dropping irrelevant columns\n",
    "2. Removal of NaN values from relevant attributes\n",
    "3. Create new dataframe consisting of only keywords and country ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"FAOLEX_ALL.csv\")\n",
    "# print(data.keys())\n",
    "# print(df.info())\n",
    "# print(keywords[:5])\n",
    "\n",
    "# Create simplified version of the dataframe by discarding irrelevant columns \n",
    "to_drop = ['Record Id', \n",
    "           'Record URL', \n",
    "           'Document URL', \n",
    "           'Title',\n",
    "           'Original title',\n",
    "           'Available website',\n",
    "           'Abstract']\n",
    "\n",
    "data.drop(to_drop, inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This results in 939 rows being dropped\n"
     ]
    }
   ],
   "source": [
    "# Remove NaN values in the Countries/Territory and Keywords attributes \n",
    "data_no_nan = data.dropna(subset=[\"Country/Territory\", \"Keywords\"])\n",
    "print(f\"This results in {len(data) - len(data_no_nan)} rows being dropped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sb/klhtpy1n7jd649__g8r7xj300000gn/T/ipykernel_25779/1041940241.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_kw['index'] = data_kw.index\n"
     ]
    }
   ],
   "source": [
    "data_kw = data_no_nan[[\"Country/Territory\",\"Keywords\"]]\n",
    "data_kw['index'] = data_kw.index\n",
    "keywords = data_kw"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pre-processing\n",
    "1. Create various pre-processing functions\n",
    "2. Perform the pre-processing on the simplified dataframe created above\n",
    "3. Stopwords removal\n",
    "4. Lemmatization (yet to be done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to remove punctuation from input text\n",
    "def remove_punctuation(text):\n",
    "    # signs = \";'-\"\n",
    "    signs = \";\"\n",
    "    #return \"\".join([i for i in text if i not in string.punctuation])\n",
    "    \n",
    "    return \"\".join([i for i in text if i not in signs])\n",
    "\n",
    "'''\n",
    "def remove_stop_words(text): \n",
    "    for word in text: \n",
    "        if word in stop_words: \n",
    "            text = text.replace(word, '')\n",
    "    return text\n",
    "'''\n",
    "\n",
    "\n",
    "# Create function to do data pre-processing (excluding stop word removal and lemmatization)\n",
    "def pre_process(text):\n",
    "    text = remove_punctuation(text)\n",
    "    # text = remove_stop_words(text)\n",
    "    text = text.lower()\n",
    "    processed = list(tokenize(text))\n",
    "    \n",
    "    return processed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing pre-processsing \n",
    "processed_kw = keywords[\"Keywords\"].map(pre_process)\n",
    "\n",
    "# Convert to a list\n",
    "processed_kw_list = processed_kw.values.tolist()\n",
    "\n",
    "# Remove stop words\n",
    "stop_words = stopwords.words('english')\n",
    "for lists in processed_kw_list:\n",
    "    for kw in lists: \n",
    "        if kw in stop_words:\n",
    "            lists.remove(kw)\n",
    "\n",
    "# Lemmatization \n",
    "# TODO figure out a way to do this \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping from word IDs to words\n",
    "# https://medium.com/analytics-vidhya/topic-modeling-using-gensim-lda-in-python-48eaa2344920\n",
    "id2word = corpora.Dictionary(processed_kw_list)\n",
    "\n",
    "# Prepare Document-Term matrix\n",
    "corpus = []\n",
    "for doc in processed_kw_list:\n",
    "    corpus.append(id2word.doc2bow(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the LDA model\n",
    "# Documentation: https://radimrehurek.com/gensim/models/ldamodel.html\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus = corpus, \n",
    "                                            id2word = id2word, \n",
    "                                            num_topics=30, \n",
    "                                            random_state=100, \n",
    "                                            passes = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(24, '0.182*\"health\" + 0.161*\"animal\" + 0.154*\"governance\" + 0.141*\"monitoring\" + 0.129*\"disasters\" + 0.045*\"production\" + 0.040*\"feed\" + 0.035*\"feedstuffs\" + 0.033*\"tourism\" + 0.023*\"welfare\"'), (14, '0.189*\"sustainable\" + 0.129*\"public\" + 0.113*\"protection\" + 0.097*\"management\" + 0.070*\"use\" + 0.067*\"environment\" + 0.056*\"biodiversity\" + 0.030*\"cultural\" + 0.028*\"heritage\" + 0.027*\"community\"'), (28, '0.380*\"land\" + 0.094*\"agricultural\" + 0.055*\"tenure\" + 0.054*\"development\" + 0.041*\"areas\" + 0.038*\"taxes\" + 0.035*\"urban\" + 0.027*\"public\" + 0.026*\"ownership\" + 0.025*\"registration\"'), (23, '0.136*\"diseases\" + 0.119*\"pests\" + 0.118*\"health\" + 0.104*\"procedural\" + 0.104*\"matters\" + 0.093*\"animal\" + 0.060*\"procedures\" + 0.060*\"sanitary\" + 0.060*\"hygiene\" + 0.038*\"international\"'), (19, '0.268*\"water\" + 0.055*\"management\" + 0.044*\"supply\" + 0.042*\"resources\" + 0.034*\"potable\" + 0.031*\"irrigation\" + 0.029*\"waterworks\" + 0.028*\"human\" + 0.026*\"surface\" + 0.026*\"groundwater\"'), (18, '0.296*\"trade\" + 0.180*\"international\" + 0.136*\"internal\" + 0.060*\"labelling\" + 0.060*\"packaging\" + 0.037*\"organic\" + 0.036*\"ecological\" + 0.026*\"nutrients\" + 0.026*\"fertilizers\" + 0.022*\"poultry\"'), (17, '0.424*\"food\" + 0.369*\"security\" + 0.158*\"nutrition\" + 0.048*\"equipment\" + 0.000*\"poverty\" + 0.000*\"gender\" + 0.000*\"social\" + 0.000*\"risk\" + 0.000*\"feeding\" + 0.000*\"school\"'), (13, '0.203*\"zone\" + 0.134*\"marine\" + 0.130*\"coastal\" + 0.089*\"management\" + 0.066*\"navigation\" + 0.054*\"harbour\" + 0.046*\"economic\" + 0.043*\"sovereignty\" + 0.038*\"sea\" + 0.035*\"maritime\"'), (21, '0.117*\"protected\" + 0.116*\"area\" + 0.107*\"conservation\" + 0.103*\"species\" + 0.100*\"protection\" + 0.079*\"wild\" + 0.067*\"ecosystem\" + 0.046*\"fauna\" + 0.044*\"habitats\" + 0.043*\"hunting\"'), (5, '0.375*\"authorization\" + 0.370*\"permit\" + 0.034*\"registration\" + 0.022*\"survey\" + 0.022*\"mapping\" + 0.022*\"inspection\" + 0.021*\"prior\" + 0.021*\"consent\" + 0.021*\"informed\" + 0.019*\"poisoning\"'), (20, '0.365*\"institution\" + 0.219*\"research\" + 0.120*\"education\" + 0.071*\"innovation\" + 0.063*\"resources\" + 0.059*\"genetic\" + 0.028*\"biodiversity\" + 0.026*\"zoning\" + 0.026*\"exploration\" + 0.009*\"procurement\"'), (15, '0.310*\"collection\" + 0.310*\"data\" + 0.310*\"reporting\" + 0.030*\"registration\" + 0.014*\"inspection\" + 0.009*\"transboundary\" + 0.007*\"movement\" + 0.006*\"medicinal\" + 0.003*\"thermal\" + 0.000*\"assessment\"'), (6, '0.280*\"system\" + 0.140*\"intervention\" + 0.140*\"early\" + 0.140*\"warning\" + 0.140*\"emergency\" + 0.028*\"practices\" + 0.027*\"desertification\" + 0.026*\"oenological\" + 0.026*\"viticulture\" + 0.022*\"beverages\"'), (29, '0.242*\"climate\" + 0.242*\"change\" + 0.163*\"access\" + 0.110*\"information\" + 0.055*\"contracts\" + 0.054*\"fees\" + 0.041*\"royalties\" + 0.025*\"environmental\" + 0.014*\"charges\" + 0.014*\"lease\"'), (25, '0.404*\"proceedings\" + 0.202*\"legal\" + 0.202*\"administrative\" + 0.072*\"institution\" + 0.022*\"settlement\" + 0.022*\"dispute\" + 0.014*\"goats\" + 0.014*\"sheep\" + 0.012*\"registration\" + 0.011*\"grazing\"'), (8, '0.147*\"rights\" + 0.132*\"indigenous\" + 0.091*\"peoples\" + 0.090*\"international\" + 0.077*\"traditional\" + 0.072*\"smallholders\" + 0.072*\"peasants\" + 0.063*\"agreement\" + 0.036*\"property\" + 0.035*\"customary\"'), (9, '0.226*\"fishing\" + 0.067*\"cooperation\" + 0.065*\"fisheries\" + 0.059*\"management\" + 0.056*\"fishery\" + 0.054*\"conservation\" + 0.051*\"marine\" + 0.039*\"vessel\" + 0.035*\"method\" + 0.035*\"gear\"'), (22, '0.357*\"forest\" + 0.163*\"management\" + 0.132*\"conservation\" + 0.063*\"timber\" + 0.041*\"reforestation\" + 0.041*\"afforestation\" + 0.030*\"extraction\" + 0.030*\"logging\" + 0.023*\"fires\" + 0.015*\"products\"'), (7, '0.165*\"declassification\" + 0.165*\"classification\" + 0.115*\"mining\" + 0.114*\"biotechnology\" + 0.096*\"crops\" + 0.085*\"grains\" + 0.085*\"cereals\" + 0.085*\"rice\" + 0.049*\"biosafety\" + 0.025*\"sugar\"'), (0, '0.178*\"farming\" + 0.154*\"products\" + 0.145*\"aquaculture\" + 0.082*\"family\" + 0.080*\"animals\" + 0.079*\"endangered\" + 0.073*\"aquatic\" + 0.044*\"plants\" + 0.038*\"mariculture\" + 0.032*\"wildlife\"')]\n"
     ]
    }
   ],
   "source": [
    "print(lda_model.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "BrokenProcessPool",
     "evalue": "A task has failed to un-serialize. Please ensure that the arguments of the function are all picklable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/Users/jonathansimonsen/opt/anaconda3/lib/python3.9/site-packages/joblib/externals/loky/process_executor.py\", line 391, in _process_worker\n    call_item = call_queue.get(block=True, timeout=timeout)\n  File \"/Users/jonathansimonsen/opt/anaconda3/lib/python3.9/multiprocessing/queues.py\", line 122, in get\n    return _ForkingPickler.loads(res)\nModuleNotFoundError: No module named 'pandas.core.indexes.numeric'\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mBrokenProcessPool\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/jonathansimonsen/Desktop/DTU/02466 Fagprojekt/Fagprojekt/js_LDA.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jonathansimonsen/Desktop/DTU/02466%20Fagprojekt/Fagprojekt/js_LDA.ipynb#X24sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpyLDAvis\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgensim_models\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jonathansimonsen/Desktop/DTU/02466%20Fagprojekt/Fagprojekt/js_LDA.ipynb#X24sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m pyLDAvis\u001b[39m.\u001b[39menable_notebook()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jonathansimonsen/Desktop/DTU/02466%20Fagprojekt/Fagprojekt/js_LDA.ipynb#X24sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m visualization \u001b[39m=\u001b[39m pyLDAvis\u001b[39m.\u001b[39;49mgensim_models\u001b[39m.\u001b[39;49mprepare(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jonathansimonsen/Desktop/DTU/02466%20Fagprojekt/Fagprojekt/js_LDA.ipynb#X24sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     lda_model, \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jonathansimonsen/Desktop/DTU/02466%20Fagprojekt/Fagprojekt/js_LDA.ipynb#X24sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     corpus,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jonathansimonsen/Desktop/DTU/02466%20Fagprojekt/Fagprojekt/js_LDA.ipynb#X24sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     id2word, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jonathansimonsen/Desktop/DTU/02466%20Fagprojekt/Fagprojekt/js_LDA.ipynb#X24sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     mds \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mmmds\u001b[39;49m\u001b[39m\"\u001b[39;49m, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jonathansimonsen/Desktop/DTU/02466%20Fagprojekt/Fagprojekt/js_LDA.ipynb#X24sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     R \u001b[39m=\u001b[39;49m \u001b[39m30\u001b[39;49m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyLDAvis/gensim_models.py:123\u001b[0m, in \u001b[0;36mprepare\u001b[0;34m(topic_model, corpus, dictionary, doc_topic_dist, **kwargs)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[39m\"\"\"Transforms the Gensim TopicModel and related corpus and dictionary into\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[39mthe data structures needed for the visualization.\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[39mSee `pyLDAvis.prepare` for **kwargs.\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    122\u001b[0m opts \u001b[39m=\u001b[39m fp\u001b[39m.\u001b[39mmerge(_extract_data(topic_model, corpus, dictionary, doc_topic_dist), kwargs)\n\u001b[0;32m--> 123\u001b[0m \u001b[39mreturn\u001b[39;00m pyLDAvis\u001b[39m.\u001b[39;49mprepare(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mopts)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyLDAvis/_prepare.py:432\u001b[0m, in \u001b[0;36mprepare\u001b[0;34m(topic_term_dists, doc_topic_dists, doc_lengths, vocab, term_frequency, R, lambda_step, mds, n_jobs, plot_opts, sort_topics, start_index)\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[39m# Quick fix for red bar width bug.  We calculate the\u001b[39;00m\n\u001b[1;32m    427\u001b[0m \u001b[39m# term frequencies internally, using the topic term distributions and the\u001b[39;00m\n\u001b[1;32m    428\u001b[0m \u001b[39m# topic frequencies, rather than using the user-supplied term frequencies.\u001b[39;00m\n\u001b[1;32m    429\u001b[0m \u001b[39m# For a detailed discussion, see: https://github.com/cpsievert/LDAvis/pull/41\u001b[39;00m\n\u001b[1;32m    430\u001b[0m term_frequency \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum(term_topic_freq, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m--> 432\u001b[0m topic_info \u001b[39m=\u001b[39m _topic_info(topic_term_dists, topic_proportion,\n\u001b[1;32m    433\u001b[0m                          term_frequency, term_topic_freq, vocab, lambda_step, R,\n\u001b[1;32m    434\u001b[0m                          n_jobs, start_index)\n\u001b[1;32m    435\u001b[0m token_table \u001b[39m=\u001b[39m _token_table(topic_info, term_topic_freq, vocab, term_frequency, start_index)\n\u001b[1;32m    436\u001b[0m topic_coordinates \u001b[39m=\u001b[39m _topic_coordinates(mds, topic_term_dists, topic_proportion, start_index)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyLDAvis/_prepare.py:273\u001b[0m, in \u001b[0;36m_topic_info\u001b[0;34m(topic_term_dists, topic_proportion, term_frequency, term_topic_freq, vocab, lambda_step, R, n_jobs, start_index)\u001b[0m\n\u001b[1;32m    262\u001b[0m     df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame({\u001b[39m'\u001b[39m\u001b[39mTerm\u001b[39m\u001b[39m'\u001b[39m: vocab[term_ix],\n\u001b[1;32m    263\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mFreq\u001b[39m\u001b[39m'\u001b[39m: term_topic_freq\u001b[39m.\u001b[39mloc[original_topic_id, term_ix],\n\u001b[1;32m    264\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mTotal\u001b[39m\u001b[39m'\u001b[39m: term_frequency[term_ix],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    267\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mloglift\u001b[39m\u001b[39m'\u001b[39m: log_lift\u001b[39m.\u001b[39mloc[original_topic_id, term_ix]\u001b[39m.\u001b[39mround(\u001b[39m4\u001b[39m),\n\u001b[1;32m    268\u001b[0m                        })\n\u001b[1;32m    269\u001b[0m     \u001b[39mreturn\u001b[39;00m df\u001b[39m.\u001b[39mreindex(columns\u001b[39m=\u001b[39m[\n\u001b[1;32m    270\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTerm\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mFreq\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mTotal\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mCategory\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mlogprob\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mloglift\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    271\u001b[0m     ])\n\u001b[0;32m--> 273\u001b[0m top_terms \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat(Parallel(n_jobs\u001b[39m=\u001b[39;49mn_jobs)\n\u001b[1;32m    274\u001b[0m                       (delayed(_find_relevance_chunks)(log_ttd, log_lift, R, ls)\n\u001b[1;32m    275\u001b[0m                       \u001b[39mfor\u001b[39;49;00m ls \u001b[39min\u001b[39;49;00m _job_chunks(lambda_seq, n_jobs)))\n\u001b[1;32m    276\u001b[0m topic_dfs \u001b[39m=\u001b[39m \u001b[39mmap\u001b[39m(topic_top_term_df, \u001b[39menumerate\u001b[39m(top_terms\u001b[39m.\u001b[39mT\u001b[39m.\u001b[39miterrows(), start_index))\n\u001b[1;32m    277\u001b[0m \u001b[39mreturn\u001b[39;00m pd\u001b[39m.\u001b[39mconcat([default_term_info] \u001b[39m+\u001b[39m \u001b[39mlist\u001b[39m(topic_dfs))\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1095\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1097\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1098\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mretrieve()\n\u001b[1;32m   1099\u001b[0m \u001b[39m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m elapsed_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_time\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    973\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    974\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, \u001b[39m'\u001b[39m\u001b[39msupports_timeout\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m--> 975\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout))\n\u001b[1;32m    976\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    977\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39mget())\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py:567\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[39m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \u001b[39mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 567\u001b[0m     \u001b[39mreturn\u001b[39;00m future\u001b[39m.\u001b[39;49mresult(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m    568\u001b[0m \u001b[39mexcept\u001b[39;00m CfTimeoutError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    569\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/concurrent/futures/_base.py:446\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    444\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    445\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[0;32m--> 446\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[1;32m    447\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    448\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/concurrent/futures/_base.py:391\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[1;32m    390\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 391\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[1;32m    392\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    393\u001b[0m         \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    394\u001b[0m         \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mBrokenProcessPool\u001b[0m: A task has failed to un-serialize. Please ensure that the arguments of the function are all picklable."
     ]
    }
   ],
   "source": [
    "# Visualizing the topics using pyLDAvis\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "visualization = pyLDAvis.gensim_models.prepare(\n",
    "    lda_model, \n",
    "    corpus,\n",
    "    id2word, \n",
    "    mds = \"mmds\", \n",
    "    R = 30)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random texting of various functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test.', 'contains', 'specific', 'words', 'jonathan,', 'gustav', 'jacob']\n",
      "test. contains specific words jonathan, gustav jacob\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "test = \"This is a test. It contains VERY specific words such as Jonathan, Gustav and Jacob\"\n",
    "test = test.lower().split()\n",
    "test = [word for word in test if not word in set(stopwords.words('english'))]\n",
    "test1 = ' '.join(test)\n",
    "print(test)\n",
    "print(test1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
